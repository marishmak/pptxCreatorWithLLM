{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "igqjd8__eHOO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import clear_output"
      ],
      "metadata": {
        "id": "LtwmOd7CVSdU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q chromadb google-generativeai python-pptx PyPDF2 pyTelegramBotAPI qdrant-client\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "K_a3Aa7vbQG-"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import chromadb\n",
        "from google.colab import userdata\n",
        "import google.generativeai as genai\n",
        "import csv\n",
        "import json\n",
        "import telebot\n",
        "from telebot import types\n",
        "import os\n",
        "import getpass\n",
        "import PyPDF2\n",
        "import re\n",
        "import time\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from qdrant_client import models, QdrantClient"
      ],
      "metadata": {
        "id": "6S8sFFTqEyFs"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ],
      "metadata": {
        "id": "gj8p0CfamEoD"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QDRANT_API = userdata.get('QDRANT-API')\n",
        "\n",
        "qdrant_client = QdrantClient(\n",
        "    url=\"https://2f1c4ffe-c4fe-4f27-a1ad-85acebc21ffe.us-east4-0.gcp.cloud.qdrant.io:6333\",\n",
        "    api_key=QDRANT_API,\n",
        ")"
      ],
      "metadata": {
        "id": "ANw6y7Hr9rLW"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Presentation creation"
      ],
      "metadata": {
        "id": "bqThGayAvt73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the embedding model from SentenceTransformers\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "\n",
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "clear_output()"
      ],
      "metadata": {
        "id": "axEUWgPpiIuZ"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_content(prompt: str) -> str:\n",
        "\n",
        "  try:\n",
        "    response = model.generate_content(prompt)\n",
        "    return response.text if hasattr(response, 'text') else \"Sorry, I couldn't generate a response.\"\n",
        "  except Exception as e:\n",
        "    return f\"There was an error generating the response: {str(e)}\"\n",
        "\n",
        "  return \"Sorry, I couldn't generate a response.\"\n",
        "\n",
        "\n",
        "def generate_prompt_for_db(text):\n",
        "  prompt_input = f\"\"\"\n",
        "You are an assistant with artificial intelligence who corrects this text, preprocesses it so that the related words are separated by a space, turns it into a coherent text, shortens it a little and outputs it convenient for explaining the topic in the presentation.\n",
        "Text:\n",
        "{text}\n",
        "Please provide output only in this format without anything else.\n",
        "Format example:\n",
        "Title:: Some title\n",
        "Text:: Some text from given text\n",
        "\"\"\"\n",
        "  content = generate_content(prompt_input)\n",
        "\n",
        "  return content\n"
      ],
      "metadata": {
        "id": "HTcTmQn0sfj9"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def database_replenishment(content, documents):\n",
        "  if 'Title:: ' in content:\n",
        "    content = content[8:].split('Title:: ')\n",
        "\n",
        "    title = ''\n",
        "    text=content\n",
        "\n",
        "    for i in content:\n",
        "      i = i.split('\\nText:: ')\n",
        "      if len(i)==2:\n",
        "        title = i[0]\n",
        "        text = i[1]\n",
        "\n",
        "      # Add page as documents to the vector database\n",
        "        documents.append({ \"title\": title, \"text\": text})\n",
        "  else:\n",
        "    documents.append({ \"title\": \"Some text\", \"text\": content})\n",
        "  return documents\n",
        "\n",
        "\n",
        "def extract_text_from_pdf(pdf_path, message, msg, COLLECTION_NAME):\n",
        "  # removing special characters and tags from the documents\n",
        "  pattern=r\"[^\\w]\"\n",
        "  length=0\n",
        "  documents=[]\n",
        "\n",
        "  with open(pdf_path, 'rb') as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "\n",
        "    for i, page in enumerate(pdf_reader.pages):\n",
        "\n",
        "      text = page.extract_text()\n",
        "\n",
        "      if i%10==0:\n",
        "        bot.edit_message_text(chat_id = message.chat.id, message_id = msg.message_id, text = f\"Please wait while the file is being written to the database. It may take some time.\\nDone...{round(i/len(pdf_reader.pages)*100, 2)}%\")\n",
        "\n",
        "      if text:\n",
        "\n",
        "        text=re.sub(pattern, \" \", text)\n",
        "        mark=True\n",
        "\n",
        "        try:\n",
        "          for i in range(5):\n",
        "            content = generate_prompt_for_db(text)\n",
        "\n",
        "            if 'error generating the response:' not in content and \"couldn't generate a response.\"  not in content:\n",
        "              documents = database_replenishment(str(content), documents)\n",
        "              mark=False\n",
        "              break\n",
        "\n",
        "          if mark:\n",
        "            documents = database_replenishment(str(text), documents)\n",
        "          mark=True\n",
        "\n",
        "        except Exception as e:\n",
        "          print('Error:', e)\n",
        "          documents = database_replenishment(str(text), documents)\n",
        "\n",
        "  qdrant_client.upload_points(\n",
        "      collection_name=COLLECTION_NAME,\n",
        "      points=[\n",
        "          models.PointStruct(\n",
        "              id=idx, vector=embedder.encode(doc[\"text\"]).tolist(), payload=doc\n",
        "          )\n",
        "          for idx, doc in enumerate(documents)\n",
        "      ],\n",
        "  )\n",
        "  bot.edit_message_text(chat_id = message.chat.id, message_id = msg.message_id, text = \"Please wait while the file is being written to the database. It may take some time.\\nDone!\")\n",
        "  print('Successful')\n",
        "  return [i['title'] for i in documents]\n"
      ],
      "metadata": {
        "id": "lXNn0u-JgqDx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_quastion_for_PP_code(topics, text, name):\n",
        "  prompt_input = f\"\"\"\n",
        "You are an assistant with artificial intelligence who generates a code for creation a PowerPoint presentation using python-pptx library based on given text. Name the pptx file like this:{name}\n",
        "Please provide output only in code format without anything else. Note that 'SlideShapes' object has no attribute 'subtitle'. Please note that the topics and texts must be spelled out explicitly (for example, text = 'Some text...')\n",
        "List of topics:\n",
        "{topics}\n",
        "List of texts from book:\n",
        "{text}\n",
        "\"\"\"\n",
        "\n",
        "  content = generate_content(prompt_input)\n",
        "  return content[10:-4]\n",
        "\n",
        "\n",
        "def generate_topics_for_section(text, titles):\n",
        "  prompt_input = f\"\"\"\n",
        "You are an artificial intelligence assistant who sets the topics that should be included in this section of the course. Create themes based on existing titles extracted from the database.\n",
        "Please provide output only in code format without anything else.\n",
        "Section:\n",
        "{text}\n",
        "Titles from database:\n",
        "{titles}\n",
        "\n",
        "Expected output format:\n",
        "topic1\n",
        "topic2\n",
        "...\n",
        "\"\"\"\n",
        "\n",
        "  content = generate_content(prompt_input)[4:-4]\n",
        "  content = content.split('\\n')\n",
        "  return content\n",
        "\n",
        "\n",
        "def extract_section_topics_from_qdrant(section, COLLECTION_NAME, titles):\n",
        "  print('Section:', section)\n",
        "  while 1:\n",
        "    topics = generate_topics_for_section(section, titles)\n",
        "\n",
        "    if 'error generating the response:' not in topics and \"couldn't generate a response.\"  not in topics:\n",
        "      break\n",
        "\n",
        "  print('Topics generated:', topics)\n",
        "\n",
        "  output=[]\n",
        "  for query in section:\n",
        "\n",
        "    # Retrieve relevant documents from ChromaDB\n",
        "    try:\n",
        "      results = qdrant_client.query_points(\n",
        "          collection_name=COLLECTION_NAME,\n",
        "          query=embedder.encode(query).tolist(),\n",
        "          limit=1,\n",
        "      ).points\n",
        "    except Exception as e:\n",
        "      print(f\"Error occured in data extraction from db: {e}\")\n",
        "      results={}\n",
        "\n",
        "\n",
        "    for result in results:\n",
        "    # Extract content from the results\n",
        "      retrieved_docs = result.payload['text']\n",
        "\n",
        "    output.append(retrieved_docs)\n",
        "\n",
        "\n",
        "\n",
        "  return output, topics\n",
        "\n",
        "\n",
        "\n",
        "def topics2code(sections, docs):\n",
        "  file_pptx = []\n",
        "  length = 0\n",
        "\n",
        "  for i, section in enumerate(sections):\n",
        "    print('Topics:', section)\n",
        "    print('Docs:', docs[i])\n",
        "\n",
        "    while len(file_pptx)<=length:\n",
        "\n",
        "      code=generate_quastion_for_PP_code(section, docs[i], f\"Lecture{i+1}.pptx\")\n",
        "\n",
        "      while 'error generating the response' in code:\n",
        "        code=generate_quastion_for_PP_code(section, docs[i], f\"Lecture{i+1}.pptx\")\n",
        "\n",
        "\n",
        "      try:\n",
        "        exec(code)\n",
        "        file_pptx.append(f\"Lecture{i+1}.pptx\")\n",
        "      except Exception as e:\n",
        "        print('Error:', e)\n",
        "\n",
        "    length+=1\n",
        "\n",
        "\n",
        "  return file_pptx\n",
        "\n"
      ],
      "metadata": {
        "id": "YHxRG_Qaw99B"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TELEGRAM_API = userdata.get('pptx_telegram_API')\n",
        "bot = telebot.TeleBot(TELEGRAM_API)\n",
        "global titles\n",
        "global COLLECTION_NAME\n",
        "titles=[]\n",
        "\n",
        "for i in qdrant_client.get_collections().collections:\n",
        "  COLLECTION_NAME=i.name\n",
        "\n",
        "\n",
        "@bot.message_handler(commands=['start', 'help'])\n",
        "def send_welcome(message):\n",
        "\n",
        "    if len(qdrant_client.get_collections().collections)==0:\n",
        "      bot.send_message(message.chat.id, \"Hi, this is a pptx_creator_bot for creating presentations based on the pdf version of the course book and lecture plan.\\nTo start creating presentations, send a pdf file with the course materials.\")\n",
        "\n",
        "    else:\n",
        "      bot.send_message(message.chat.id, \"Hi, this is a pptx_creator_bot for creating presentations based on the pdf version of the course book and lecture plan.\\nYou already have a pdf file in the database.\\nTo continue working with the file, send topics for creating presentations, and to replace the file with another one, send a new pdf file with course materials, but the data from the previous file will be deleted.\")\n",
        "\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=['document'])\n",
        "def handle_pdf(message):\n",
        "    if len(qdrant_client.get_collections().collections)!=0:\n",
        "      client.delete_collection(collection_name=f\"{COLLECTION_NAME}\")\n",
        "    try:\n",
        "\n",
        "\n",
        "        # Download the PDF file\n",
        "        file_info = bot.get_file(message.document.file_id)\n",
        "        downloaded_file = bot.download_file(file_info.file_path)\n",
        "        COLLECTION_NAME = str(message.document.file_id)\n",
        "\n",
        "        qdrant_client.create_collection(\n",
        "            collection_name=COLLECTION_NAME,\n",
        "            vectors_config=models.VectorParams(\n",
        "                size=embedder.get_sentence_embedding_dimension(),\n",
        "                distance=models.Distance.COSINE,\n",
        "            ),\n",
        "        )\n",
        "\n",
        "        pdf_path = 'book.pdf'\n",
        "        # Save the PDF file temporarily\n",
        "        with open(pdf_path, 'wb') as new_file:\n",
        "            new_file.write(downloaded_file)\n",
        "\n",
        "        # Extract text from the PDF\n",
        "        msg = bot.send_message(message.chat.id, \"Please wait while the file is being written to the database. It may take some time.\")\n",
        "\n",
        "        titles = extract_text_from_pdf(pdf_path, message, msg, COLLECTION_NAME)\n",
        "        os.remove('book.pdf')\n",
        "\n",
        "\n",
        "        # Wait for the user to send text\n",
        "        bot.reply_to(message, f\"{len(titles)} documents have been successfully saved in the database.\")\n",
        "        bot.reply_to(message, \"Please provide me the course content by highlighting each lecture in a separate line so that I can include it in pptx.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.reply_to(message, f\"Error: {e}\")\n",
        "\n",
        "\n",
        "@bot.message_handler(content_types=['text'])\n",
        "def handle_text(message):\n",
        "\n",
        "  if len(qdrant_client.get_collections().collections)==0:\n",
        "    bot.reply_to(message, \"The database does not contain data about the book, so first send the pdf file, and then enter the topics of the lectures.\")\n",
        "  else:\n",
        "\n",
        "    try:\n",
        "        # Get the text from the user\n",
        "        sections = message.text\n",
        "        bot.reply_to(message, \"Please wait until the bot sends you the pptx files.\")\n",
        "        sections = sections.split('\\n')\n",
        "\n",
        "        topics=[]\n",
        "        docs = []\n",
        "\n",
        "        # Create a PPTX file\n",
        "        for section in sections:\n",
        "          outputs, queries = extract_section_topics_from_qdrant(section, COLLECTION_NAME, titles)\n",
        "          topics.append(queries)\n",
        "          docs.append(outputs)\n",
        "\n",
        "        presents = topics2code(topics, docs)\n",
        "\n",
        "        # Send the PPTX file back to the user\n",
        "        for i in presents:\n",
        "          with open(i, 'rb') as pptx_file:\n",
        "              bot.send_document(message.chat.id, pptx_file)\n",
        "\n",
        "        for i in presents:\n",
        "          os.remove(i)\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        bot.reply_to(message, f\"Error: {e}\")\n",
        "\n",
        "\n",
        "# Run the bot\n",
        "bot.polling(non_stop=True, interval=0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZxB9a9UJBDX5",
        "outputId": "b93cb369-24c2-44cf-af08-4172850089e1"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Section: Representation of images and videos (Computer representation, Rescaling/manipulating images)\n",
            "Topics generated: ['Image Representation', 'Image Data Structures', 'Pixel Manipulation', 'Image Rescaling', 'Image Filtering', 'Image Enhancement', 'Image Compression', 'Video Representation', 'Video Data Structures', 'Frame Processing', 'Video Compression']\n",
            "Section: Image Classification (Loss Functions, Backpropagation)\n",
            "Topics generated: ['Image Classification Fundamentals', 'Loss Functions for Image Classification', 'Backpropagation in Image Classification']\n",
            "Section: Neural Networks (Training)\n",
            "Topics generated: ['Backpropagation', 'Gradient Descent', 'Optimization Algorithms', 'Regularization', 'Hyperparameter Tuning', 'Early Stopping', 'Learning Rate Scheduling', 'Batch Normalization', 'Dropout', 'Data Augmentation', 'Overfitting and Underfitting', 'Model Evaluation']\n",
            "Section: Convolutional Neural Networks  (Training, Architectures)\n",
            "Topics generated: ['Convolutional Neural Network Architectures', 'Training Convolutional Neural Networks']\n",
            "Section: Recurrent Neural Networks (Training, Architectures)\n",
            "Topics generated: ['Recurrent Neural Network Architectures', 'Training Recurrent Neural Networks', 'Backpropagation Through Time', 'Vanishing and Exploding Gradients', 'Long Short-Term Memory (LSTM) Networks', 'Gated Recurrent Units (GRUs)', 'Recurrent Neural Networks for Sequence Modeling', 'Recurrent Neural Networks for Natural Language Processing', 'Recurrent Neural Networks for Time Series Analysis']\n",
            "Section: Image Segmentation and object detection (Techniques)\n",
            "Topics generated: ['Image Segmentation Techniques', 'Object Detection Techniques', 'Deep Learning for Image Segmentation', 'Deep Learning for Object Detection', 'Traditional Image Segmentation Methods', 'Traditional Object Detection Methods', 'Semantic Segmentation', 'Instance Segmentation', 'Image Segmentation Evaluation Metrics', 'Object Detection Evaluation Metrics', 'Applications of Image Segmentation', 'Applications of Object Detection']\n",
            "Topics: ['Image Representation', 'Image Data Structures', 'Pixel Manipulation', 'Image Rescaling', 'Image Filtering', 'Image Enhancement', 'Image Compression', 'Video Representation', 'Video Data Structures', 'Frame Processing', 'Video Compression']\n",
            "Docs: ['This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Specificity, also known as the true negative rate, is the percentage of correctly classified negative instances. It can be calculated as: Specificity = TN / (TN + FP), where TN is the number of true negatives and FP is the number of false positives.  \\n\\nThe F-beta measure is a weighted average of precision and recall, allowing different emphasis on each metric. It is calculated as: F-beta = (1 + beta^2) * (Precision * Recall) / (beta^2 * Precision + Recall). \\n\\nThere are three commonly used F-beta measures: F1, F2, and F0.5. The F1 measure gives equal weight to precision and recall, while F2 prioritizes recall and F0.5 prioritizes precision.\\n\\nFor example, Figure 8.27 shows the precision, recall, and F1 values for various threshold values in a template matching example. The specificity and accuracy values are not shown as they are close to 1.0 for all threshold values. A threshold value of 0.992 would have resulted in perfect performance. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'OpenCV is used in a wide range of applications, including:\\n- Face Recognition \\n- Object Detection\\n- Motion Tracking\\n- Image Stitching\\n- Medical Imaging \\n- Robotics\\n- Self-Driving Cars \\n- Security Systems\\n- Surveillance\\n- Augmented Reality \\n- Virtual Reality  \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Error: 'NoneType' object has no attribute 'text'\n",
            "Error: name 'topics' is not defined\n",
            "Error: name 'texts' is not defined\n",
            "Error: name 'topics' is not defined\n",
            "Error: name 'topics' is not defined\n",
            "Error: unterminated string literal (detected at line 13) (<string>, line 13)\n",
            "Error: unterminated string literal (detected at line 13) (<string>, line 13)\n",
            "Error: unterminated string literal (detected at line 14) (<string>, line 14)\n",
            "Error: 'SlideShapes' object has no attribute 'subtitle'\n",
            "Error: unterminated string literal (detected at line 21) (<string>, line 21)\n",
            "Topics: ['Image Classification Fundamentals', 'Loss Functions for Image Classification', 'Backpropagation in Image Classification']\n",
            "Docs: ['The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'Specificity, also known as the true negative rate, is the percentage of correctly classified negative instances. It can be calculated as: Specificity = TN / (TN + FP), where TN is the number of true negatives and FP is the number of false positives.  \\n\\nThe F-beta measure is a weighted average of precision and recall, allowing different emphasis on each metric. It is calculated as: F-beta = (1 + beta^2) * (Precision * Recall) / (beta^2 * Precision + Recall). \\n\\nThere are three commonly used F-beta measures: F1, F2, and F0.5. The F1 measure gives equal weight to precision and recall, while F2 prioritizes recall and F0.5 prioritizes precision.\\n\\nFor example, Figure 8.27 shows the precision, recall, and F1 values for various threshold values in a template matching example. The specificity and accuracy values are not shown as they are close to 1.0 for all threshold values. A threshold value of 0.992 would have resulted in perfect performance. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'Specificity, also known as the true negative rate, is the percentage of correctly classified negative instances. It can be calculated as: Specificity = TN / (TN + FP), where TN is the number of true negatives and FP is the number of false positives.  \\n\\nThe F-beta measure is a weighted average of precision and recall, allowing different emphasis on each metric. It is calculated as: F-beta = (1 + beta^2) * (Precision * Recall) / (beta^2 * Precision + Recall). \\n\\nThere are three commonly used F-beta measures: F1, F2, and F0.5. The F1 measure gives equal weight to precision and recall, while F2 prioritizes recall and F0.5 prioritizes precision.\\n\\nFor example, Figure 8.27 shows the precision, recall, and F1 values for various threshold values in a template matching example. The specificity and accuracy values are not shown as they are close to 1.0 for all threshold values. A threshold value of 0.992 would have resulted in perfect performance. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', ' Viola-Jones object detection relies on a cascade of features for recognition.  These features are simple and efficient to calculate, and they effectively capture characteristics of the object being detected.  For instance, the first two features used in face detection aim to capture the contrast between cheek and eye regions, and the brightness difference between the space between eyes and the eyes themselves.\\n\\nTo compute these features efficiently, the integral image is introduced. This image stores the sum of all pixels with smaller coordinates than a given pixel. Using the integral image, the sum of pixels within any rectangular region can be calculated with just four operations: two additions and two subtractions.  This allows for rapid computation of features at any scale, which is crucial for detecting objects at different sizes.  OpenCV provides functions for calculating integral images in both normal and 45-degree orientations. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Topics: ['Backpropagation', 'Gradient Descent', 'Optimization Algorithms', 'Regularization', 'Hyperparameter Tuning', 'Early Stopping', 'Learning Rate Scheduling', 'Batch Normalization', 'Dropout', 'Data Augmentation', 'Overfitting and Underfitting', 'Model Evaluation']\n",
            "Docs: [\"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"The Sobel operator is a popular edge detection technique used in computer vision. It computes the gradient magnitude and orientation of an image using two partial derivatives. The Sobel operator effectively incorporates smoothing within the partial derivative convolution masks. It also considers points that are slightly separated, which significantly improves its performance on real gray-scale images compared to the Roberts operator. In OpenCV, the Sobel partial derivatives can be computed as follows: `Mat horizontal_derivative, vertical_derivative; Sobel(gray_image, horizontal_derivative, CV_32F, 1, 0); Sobel(gray_image, vertical_derivative, CV_32F, 0, 1);`  Both Sobel and Prewitt operators are centred on a particular pixel, hence they don't cause any shift in the position of the detected edges unlike the Roberts operator. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', ' Viola-Jones object detection relies on a cascade of features for recognition.  These features are simple and efficient to calculate, and they effectively capture characteristics of the object being detected.  For instance, the first two features used in face detection aim to capture the contrast between cheek and eye regions, and the brightness difference between the space between eyes and the eyes themselves.\\n\\nTo compute these features efficiently, the integral image is introduced. This image stores the sum of all pixels with smaller coordinates than a given pixel. Using the integral image, the sum of pixels within any rectangular region can be calculated with just four operations: two additions and two subtractions.  This allows for rapid computation of features at any scale, which is crucial for detecting objects at different sizes.  OpenCV provides functions for calculating integral images in both normal and 45-degree orientations. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Error: list index out of range\n",
            "Error: 'SlideShapes' object has no attribute 'subtitle'\n",
            "Error: list index out of range\n",
            "Error: 'SlideShapes' object has no attribute 'subtitle'\n",
            "Error: 'SlideShapes' object has no attribute 'subtitle'\n",
            "Topics: ['Convolutional Neural Network Architectures', 'Training Convolutional Neural Networks']\n",
            "Docs: ['64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'OpenCV is used in a wide range of applications, including:\\n- Face Recognition \\n- Object Detection\\n- Motion Tracking\\n- Image Stitching\\n- Medical Imaging \\n- Robotics\\n- Self-Driving Cars \\n- Security Systems\\n- Surveillance\\n- Augmented Reality \\n- Virtual Reality  \\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"The Sobel operator is a popular edge detection technique used in computer vision. It computes the gradient magnitude and orientation of an image using two partial derivatives. The Sobel operator effectively incorporates smoothing within the partial derivative convolution masks. It also considers points that are slightly separated, which significantly improves its performance on real gray-scale images compared to the Roberts operator. In OpenCV, the Sobel partial derivatives can be computed as follows: `Mat horizontal_derivative, vertical_derivative; Sobel(gray_image, horizontal_derivative, CV_32F, 1, 0); Sobel(gray_image, vertical_derivative, CV_32F, 0, 1);`  Both Sobel and Prewitt operators are centred on a particular pixel, hence they don't cause any shift in the position of the detected edges unlike the Roberts operator. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', ' Viola-Jones object detection relies on a cascade of features for recognition.  These features are simple and efficient to calculate, and they effectively capture characteristics of the object being detected.  For instance, the first two features used in face detection aim to capture the contrast between cheek and eye regions, and the brightness difference between the space between eyes and the eyes themselves.\\n\\nTo compute these features efficiently, the integral image is introduced. This image stores the sum of all pixels with smaller coordinates than a given pixel. Using the integral image, the sum of pixels within any rectangular region can be calculated with just four operations: two additions and two subtractions.  This allows for rapid computation of features at any scale, which is crucial for detecting objects at different sizes.  OpenCV provides functions for calculating integral images in both normal and 45-degree orientations. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'To convert RGB to HLS, we use the following formulas, assuming R, G, and B have been normalized to between 0.0 and 1.0:\\n\\nL = (Max(R, G, B) + Min(R, G, B)) / 2\\nS = (Max(R, G, B) - Min(R, G, B)) / (Max(R, G, B) + Min(R, G, B)) if L <= 0.5\\nS = (Max(R, G, B) - Min(R, G, B)) / (2 - Max(R, G, B) - Min(R, G, B)) if L > 0.5\\n\\nH = 60 * (G - B) / S if R == Max(R, G, B)\\nH = 120 + 60 * (B - R) / S if G == Max(R, G, B)\\nH = 240 + 60 * (R - G) / S if B == Max(R, G, B)\\n\\nFrom the above formulas, the L and S values will be between 0.0 and 1.0, and the H value should range between 0.0 and 360.0. We will need to add 360.0 to any H value which is less than 0.0 and subtract 360.0 from any value greater than or equal to 360.0. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Error: unterminated string literal (detected at line 158) (<string>, line 158)\n",
            "Topics: ['Recurrent Neural Network Architectures', 'Training Recurrent Neural Networks', 'Backpropagation Through Time', 'Vanishing and Exploding Gradients', 'Long Short-Term Memory (LSTM) Networks', 'Gated Recurrent Units (GRUs)', 'Recurrent Neural Networks for Sequence Modeling', 'Recurrent Neural Networks for Natural Language Processing', 'Recurrent Neural Networks for Time Series Analysis']\n",
            "Docs: ['This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"The Sobel operator is a popular edge detection technique used in computer vision. It computes the gradient magnitude and orientation of an image using two partial derivatives. The Sobel operator effectively incorporates smoothing within the partial derivative convolution masks. It also considers points that are slightly separated, which significantly improves its performance on real gray-scale images compared to the Roberts operator. In OpenCV, the Sobel partial derivatives can be computed as follows: `Mat horizontal_derivative, vertical_derivative; Sobel(gray_image, horizontal_derivative, CV_32F, 1, 0); Sobel(gray_image, vertical_derivative, CV_32F, 0, 1);`  Both Sobel and Prewitt operators are centred on a particular pixel, hence they don't cause any shift in the position of the detected edges unlike the Roberts operator. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', ' Viola-Jones object detection relies on a cascade of features for recognition.  These features are simple and efficient to calculate, and they effectively capture characteristics of the object being detected.  For instance, the first two features used in face detection aim to capture the contrast between cheek and eye regions, and the brightness difference between the space between eyes and the eyes themselves.\\n\\nTo compute these features efficiently, the integral image is introduced. This image stores the sum of all pixels with smaller coordinates than a given pixel. Using the integral image, the sum of pixels within any rectangular region can be calculated with just four operations: two additions and two subtractions.  This allows for rapid computation of features at any scale, which is crucial for detecting objects at different sizes.  OpenCV provides functions for calculating integral images in both normal and 45-degree orientations. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'To convert RGB to HLS, we use the following formulas, assuming R, G, and B have been normalized to between 0.0 and 1.0:\\n\\nL = (Max(R, G, B) + Min(R, G, B)) / 2\\nS = (Max(R, G, B) - Min(R, G, B)) / (Max(R, G, B) + Min(R, G, B)) if L <= 0.5\\nS = (Max(R, G, B) - Min(R, G, B)) / (2 - Max(R, G, B) - Min(R, G, B)) if L > 0.5\\n\\nH = 60 * (G - B) / S if R == Max(R, G, B)\\nH = 120 + 60 * (B - R) / S if G == Max(R, G, B)\\nH = 240 + 60 * (R - G) / S if B == Max(R, G, B)\\n\\nFrom the above formulas, the L and S values will be between 0.0 and 1.0, and the H value should range between 0.0 and 360.0. We will need to add 360.0 to any H value which is less than 0.0 and subtract 360.0 from any value greater than or equal to 360.0. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Error: 'SlideShapes' object has no attribute 'subtitle'\n",
            "Error: slide index out of range\n",
            "Topics: ['Image Segmentation Techniques', 'Object Detection Techniques', 'Deep Learning for Image Segmentation', 'Deep Learning for Object Detection', 'Traditional Image Segmentation Methods', 'Traditional Object Detection Methods', 'Semantic Segmentation', 'Instance Segmentation', 'Image Segmentation Evaluation Metrics', 'Object Detection Evaluation Metrics', 'Applications of Image Segmentation', 'Applications of Object Detection']\n",
            "Docs: ['The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'To convert RGB to HLS, we use the following formulas, assuming R, G, and B have been normalized to between 0.0 and 1.0:\\n\\nL = (Max(R, G, B) + Min(R, G, B)) / 2\\nS = (Max(R, G, B) - Min(R, G, B)) / (Max(R, G, B) + Min(R, G, B)) if L <= 0.5\\nS = (Max(R, G, B) - Min(R, G, B)) / (2 - Max(R, G, B) - Min(R, G, B)) if L > 0.5\\n\\nH = 60 * (G - B) / S if R == Max(R, G, B)\\nH = 120 + 60 * (B - R) / S if G == Max(R, G, B)\\nH = 240 + 60 * (R - G) / S if B == Max(R, G, B)\\n\\nFrom the above formulas, the L and S values will be between 0.0 and 1.0, and the H value should range between 0.0 and 360.0. We will need to add 360.0 to any H value which is less than 0.0 and subtract 360.0 from any value greater than or equal to 360.0. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1452.85ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1118.95ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2055.65ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1421.63ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1297.03ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 710.29ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1524.08ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1170.71ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1147.79ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1424.76ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1318.45ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1446.80ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1878.59ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 5838.25ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 789.15ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1424.37ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2161.92ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2436.82ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 815.02ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1724.15ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1194.68ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1319.76ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1297.85ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1222.50ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1626.28ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1968.67ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1598.16ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2133.69ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1454.44ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1297.70ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2947.63ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1628.77ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1928.50ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1853.20ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1401.60ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1533.09ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 917.39ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1376.78ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1774.96ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1297.51ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3275.40ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1576.09ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2128.90ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2355.34ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1346.74ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2834.41ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1348.86ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1681.74ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1499.46ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1496.68ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2661.51ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2671.17ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1119.16ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2182.78ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1172.36ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 943.54ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2892.79ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1956.58ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1780.80ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2086.00ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1595.11ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1473.87ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2716.84ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3707.95ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1684.38ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 7376.03ms\n",
            "/usr/local/lib/python3.10/dist-packages/PyPDF2/_cmap.py:151: PdfReadWarning: Advanced encoding [] not implemented yet\n",
            "  warnings.warn(\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3626.54ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1450.27ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1473.04ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1447.50ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 4706.94ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3153.56ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1422.14ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 4808.03ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1630.96ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2089.63ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1480.05ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1472.43ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1221.77ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2569.16ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 4009.58ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1703.23ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1475.76ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 965.33ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 790.63ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1756.01ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2134.31ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2335.97ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1474.26ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2154.79ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 2233.54ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1708.30ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1546.96ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 3708.53ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1367.19ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1599.98ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1571.28ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1043.81ms\n",
            "WARNING:tornado.access:429 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (127.0.0.1) 1116.87ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successful\n",
            "Section: Representation of images and videos (Computer representation, Rescaling/manipulating images)\n",
            "Topics generated: ['Image Representation', 'Pixel-Based Representation', 'Color Models', 'Image Compression', 'Image Rescaling', 'Image Manipulation', 'Geometric Transformations', 'Filtering and Enhancement', 'Video Representation', 'Video Compression']\n",
            "Section: Image Classification (Loss Functions, Backpropagation)\n",
            "Topics generated: ['Image Classification Fundamentals', 'Loss Functions in Image Classification', 'Backpropagation for Image Classification']\n",
            "Section: Neural Networks (Training)\n",
            "Topics generated: ['Training Neural Networks', 'Backpropagation Algorithm', 'Gradient Descent and its Variants', 'Optimization Techniques', 'Learning Rate and its Impact', 'Regularization Methods', 'Overfitting and Underfitting', 'Hyperparameter Tuning', 'Early Stopping', 'Data Augmentation', 'Batch Normalization', 'Transfer Learning']\n",
            "Section: Convolutional Neural Networks  (Training, Architectures)\n",
            "Topics generated: ['Convolutional Neural Network Architectures', 'Training Convolutional Neural Networks']\n",
            "Section: Recurrent Neural Networks (Training, Architectures)\n",
            "Topics generated: ['Recurrent Neural Network Architectures', 'Training Recurrent Neural Networks']\n",
            "Section: Image Segmentation and object detection (Techniques)\n",
            "Topics generated: ['Image Segmentation Techniques', 'Object Detection Techniques', 'Deep Learning for Image Segmentation', 'Deep Learning for Object Detection', 'Traditional Image Segmentation Methods', 'Traditional Object Detection Methods', 'Semantic Segmentation', 'Instance Segmentation', 'Panoptic Segmentation', 'Object Localization', 'Object Classification', 'Image Preprocessing for Segmentation and Detection', 'Evaluation Metrics for Segmentation and Detection', 'Applications of Image Segmentation and Object Detection']\n",
            "Topics: ['Image Representation', 'Pixel-Based Representation', 'Color Models', 'Image Compression', 'Image Rescaling', 'Image Manipulation', 'Geometric Transformations', 'Filtering and Enhancement', 'Video Representation', 'Video Compression']\n",
            "Docs: ['This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Specificity, also known as the true negative rate, is the percentage of correctly classified negative instances. It can be calculated as: Specificity = TN / (TN + FP), where TN is the number of true negatives and FP is the number of false positives.  \\n\\nThe F-beta measure is a weighted average of precision and recall, allowing different emphasis on each metric. It is calculated as: F-beta = (1 + beta^2) * (Precision * Recall) / (beta^2 * Precision + Recall). \\n\\nThere are three commonly used F-beta measures: F1, F2, and F0.5. The F1 measure gives equal weight to precision and recall, while F2 prioritizes recall and F0.5 prioritizes precision.\\n\\nFor example, Figure 8.27 shows the precision, recall, and F1 values for various threshold values in a template matching example. The specificity and accuracy values are not shown as they are close to 1.0 for all threshold values. A threshold value of 0.992 would have resulted in perfect performance. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'OpenCV is used in a wide range of applications, including:\\n- Face Recognition \\n- Object Detection\\n- Motion Tracking\\n- Image Stitching\\n- Medical Imaging \\n- Robotics\\n- Self-Driving Cars \\n- Security Systems\\n- Surveillance\\n- Augmented Reality \\n- Virtual Reality  \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Error: unterminated string literal (detected at line 17) (<string>, line 17)\n",
            "Error: 'SlideShapes' object has no attribute 'subtitle'\n",
            "Error: unterminated string literal (detected at line 13) (<string>, line 13)\n",
            "Error: unterminated string literal (detected at line 14) (<string>, line 14)\n",
            "Topics: ['Image Classification Fundamentals', 'Loss Functions in Image Classification', 'Backpropagation for Image Classification']\n",
            "Docs: ['The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'Specificity, also known as the true negative rate, is the percentage of correctly classified negative instances. It can be calculated as: Specificity = TN / (TN + FP), where TN is the number of true negatives and FP is the number of false positives.  \\n\\nThe F-beta measure is a weighted average of precision and recall, allowing different emphasis on each metric. It is calculated as: F-beta = (1 + beta^2) * (Precision * Recall) / (beta^2 * Precision + Recall). \\n\\nThere are three commonly used F-beta measures: F1, F2, and F0.5. The F1 measure gives equal weight to precision and recall, while F2 prioritizes recall and F0.5 prioritizes precision.\\n\\nFor example, Figure 8.27 shows the precision, recall, and F1 values for various threshold values in a template matching example. The specificity and accuracy values are not shown as they are close to 1.0 for all threshold values. A threshold value of 0.992 would have resulted in perfect performance. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'Specificity, also known as the true negative rate, is the percentage of correctly classified negative instances. It can be calculated as: Specificity = TN / (TN + FP), where TN is the number of true negatives and FP is the number of false positives.  \\n\\nThe F-beta measure is a weighted average of precision and recall, allowing different emphasis on each metric. It is calculated as: F-beta = (1 + beta^2) * (Precision * Recall) / (beta^2 * Precision + Recall). \\n\\nThere are three commonly used F-beta measures: F1, F2, and F0.5. The F1 measure gives equal weight to precision and recall, while F2 prioritizes recall and F0.5 prioritizes precision.\\n\\nFor example, Figure 8.27 shows the precision, recall, and F1 values for various threshold values in a template matching example. The specificity and accuracy values are not shown as they are close to 1.0 for all threshold values. A threshold value of 0.992 would have resulted in perfect performance. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', ' Viola-Jones object detection relies on a cascade of features for recognition.  These features are simple and efficient to calculate, and they effectively capture characteristics of the object being detected.  For instance, the first two features used in face detection aim to capture the contrast between cheek and eye regions, and the brightness difference between the space between eyes and the eyes themselves.\\n\\nTo compute these features efficiently, the integral image is introduced. This image stores the sum of all pixels with smaller coordinates than a given pixel. Using the integral image, the sum of pixels within any rectangular region can be calculated with just four operations: two additions and two subtractions.  This allows for rapid computation of features at any scale, which is crucial for detecting objects at different sizes.  OpenCV provides functions for calculating integral images in both normal and 45-degree orientations. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Topics: ['Training Neural Networks', 'Backpropagation Algorithm', 'Gradient Descent and its Variants', 'Optimization Techniques', 'Learning Rate and its Impact', 'Regularization Methods', 'Overfitting and Underfitting', 'Hyperparameter Tuning', 'Early Stopping', 'Data Augmentation', 'Batch Normalization', 'Transfer Learning']\n",
            "Docs: [\"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"The Sobel operator is a popular edge detection technique used in computer vision. It computes the gradient magnitude and orientation of an image using two partial derivatives. The Sobel operator effectively incorporates smoothing within the partial derivative convolution masks. It also considers points that are slightly separated, which significantly improves its performance on real gray-scale images compared to the Roberts operator. In OpenCV, the Sobel partial derivatives can be computed as follows: `Mat horizontal_derivative, vertical_derivative; Sobel(gray_image, horizontal_derivative, CV_32F, 1, 0); Sobel(gray_image, vertical_derivative, CV_32F, 0, 1);`  Both Sobel and Prewitt operators are centred on a particular pixel, hence they don't cause any shift in the position of the detected edges unlike the Roberts operator. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', ' Viola-Jones object detection relies on a cascade of features for recognition.  These features are simple and efficient to calculate, and they effectively capture characteristics of the object being detected.  For instance, the first two features used in face detection aim to capture the contrast between cheek and eye regions, and the brightness difference between the space between eyes and the eyes themselves.\\n\\nTo compute these features efficiently, the integral image is introduced. This image stores the sum of all pixels with smaller coordinates than a given pixel. Using the integral image, the sum of pixels within any rectangular region can be calculated with just four operations: two additions and two subtractions.  This allows for rapid computation of features at any scale, which is crucial for detecting objects at different sizes.  OpenCV provides functions for calculating integral images in both normal and 45-degree orientations. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Topics: ['Convolutional Neural Network Architectures', 'Training Convolutional Neural Networks']\n",
            "Docs: ['64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'OpenCV is used in a wide range of applications, including:\\n- Face Recognition \\n- Object Detection\\n- Motion Tracking\\n- Image Stitching\\n- Medical Imaging \\n- Robotics\\n- Self-Driving Cars \\n- Security Systems\\n- Surveillance\\n- Augmented Reality \\n- Virtual Reality  \\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"The Sobel operator is a popular edge detection technique used in computer vision. It computes the gradient magnitude and orientation of an image using two partial derivatives. The Sobel operator effectively incorporates smoothing within the partial derivative convolution masks. It also considers points that are slightly separated, which significantly improves its performance on real gray-scale images compared to the Roberts operator. In OpenCV, the Sobel partial derivatives can be computed as follows: `Mat horizontal_derivative, vertical_derivative; Sobel(gray_image, horizontal_derivative, CV_32F, 1, 0); Sobel(gray_image, vertical_derivative, CV_32F, 0, 1);`  Both Sobel and Prewitt operators are centred on a particular pixel, hence they don't cause any shift in the position of the detected edges unlike the Roberts operator. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', ' Viola-Jones object detection relies on a cascade of features for recognition.  These features are simple and efficient to calculate, and they effectively capture characteristics of the object being detected.  For instance, the first two features used in face detection aim to capture the contrast between cheek and eye regions, and the brightness difference between the space between eyes and the eyes themselves.\\n\\nTo compute these features efficiently, the integral image is introduced. This image stores the sum of all pixels with smaller coordinates than a given pixel. Using the integral image, the sum of pixels within any rectangular region can be calculated with just four operations: two additions and two subtractions.  This allows for rapid computation of features at any scale, which is crucial for detecting objects at different sizes.  OpenCV provides functions for calculating integral images in both normal and 45-degree orientations. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'To convert RGB to HLS, we use the following formulas, assuming R, G, and B have been normalized to between 0.0 and 1.0:\\n\\nL = (Max(R, G, B) + Min(R, G, B)) / 2\\nS = (Max(R, G, B) - Min(R, G, B)) / (Max(R, G, B) + Min(R, G, B)) if L <= 0.5\\nS = (Max(R, G, B) - Min(R, G, B)) / (2 - Max(R, G, B) - Min(R, G, B)) if L > 0.5\\n\\nH = 60 * (G - B) / S if R == Max(R, G, B)\\nH = 120 + 60 * (B - R) / S if G == Max(R, G, B)\\nH = 240 + 60 * (R - G) / S if B == Max(R, G, B)\\n\\nFrom the above formulas, the L and S values will be between 0.0 and 1.0, and the H value should range between 0.0 and 360.0. We will need to add 360.0 to any H value which is less than 0.0 and subtract 360.0 from any value greater than or equal to 360.0. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Error: 'SlideShapes' object has no attribute 'subtitle'\n",
            "Topics: ['Recurrent Neural Network Architectures', 'Training Recurrent Neural Networks']\n",
            "Docs: ['This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', '180 A Practical IntroductiontoComputer Vision with OpenCV Figure9 11 ThemovingpixelsdetectedbytheGMM left togetherwiththerawmovingpixels white  and detected shadow points  grey   The results are quite good  although it is notable  from the moving object pixels shown on the left  that the very dark shadow under the van has been classified incorrectly andthatthereisareasonableamountofnoiseclassifiedincorrectlyasmovingobjectpixels Theoriginal images are from the PETS 2000 dataset  Reproduced by permission of Dr  James Ferryman  University of Reading 9 2 Tracking Trackingofanobject ormultipleobjects invideoisanimportantproblemincomputervision  Maggio and Cavallaro  2010   It is employed in visual surveillance  sports video analysis  vehicle guidance systems obstacle avoidance  and soon  Visual tracking is usually not simple  as theobject s  to be tracked  1  may be undergoing complex motion relative to the camera  e g  a car in front turning off the road   2  may change shape  e g  a person s appearance changes hugely when moving as the arms and legs and head move relative to the trunk   3  may be fully or partially occluded at times  e g  a truck passes between a pedestrian and the camera   4  may change appearance due to lighting or weather  e g  lights being turned on  or the sun going behind a cloud  or rainstarting   5  may physically change appearance  e g  aperson who takes off a jacket or ahat   Asaresult visualtrackingisaverydifficultproblemandhasbeenapproachedinavariety of different ways  We consider anumber of themin this section  1  Exhaustive search  see Section 9 2 1   where an image  a template  is tracked from frame to framesearching for the best match in each frame  2  Mean shift  see Section 9 2 2   where a histogram representation of the object is used for the comparison and a gradient ascent descent method is used to reduce the amount of comparisons required  3  Optical flow  see Section 9 2 3  is a technique for determining the direction in which all pixels orsparsefeatures aremovingfromframetoframe Thisisoftenemployedtoguide trackers  We consider the Farneback method ', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"The Sobel operator is a popular edge detection technique used in computer vision. It computes the gradient magnitude and orientation of an image using two partial derivatives. The Sobel operator effectively incorporates smoothing within the partial derivative convolution masks. It also considers points that are slightly separated, which significantly improves its performance on real gray-scale images compared to the Roberts operator. In OpenCV, the Sobel partial derivatives can be computed as follows: `Mat horizontal_derivative, vertical_derivative; Sobel(gray_image, horizontal_derivative, CV_32F, 1, 0); Sobel(gray_image, vertical_derivative, CV_32F, 0, 1);`  Both Sobel and Prewitt operators are centred on a particular pixel, hence they don't cause any shift in the position of the detected edges unlike the Roberts operator. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', ' Viola-Jones object detection relies on a cascade of features for recognition.  These features are simple and efficient to calculate, and they effectively capture characteristics of the object being detected.  For instance, the first two features used in face detection aim to capture the contrast between cheek and eye regions, and the brightness difference between the space between eyes and the eyes themselves.\\n\\nTo compute these features efficiently, the integral image is introduced. This image stores the sum of all pixels with smaller coordinates than a given pixel. Using the integral image, the sum of pixels within any rectangular region can be calculated with just four operations: two additions and two subtractions.  This allows for rapid computation of features at any scale, which is crucial for detecting objects at different sizes.  OpenCV provides functions for calculating integral images in both normal and 45-degree orientations. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'To convert RGB to HLS, we use the following formulas, assuming R, G, and B have been normalized to between 0.0 and 1.0:\\n\\nL = (Max(R, G, B) + Min(R, G, B)) / 2\\nS = (Max(R, G, B) - Min(R, G, B)) / (Max(R, G, B) + Min(R, G, B)) if L <= 0.5\\nS = (Max(R, G, B) - Min(R, G, B)) / (2 - Max(R, G, B) - Min(R, G, B)) if L > 0.5\\n\\nH = 60 * (G - B) / S if R == Max(R, G, B)\\nH = 120 + 60 * (B - R) / S if G == Max(R, G, B)\\nH = 240 + 60 * (R - G) / S if B == Max(R, G, B)\\n\\nFrom the above formulas, the L and S values will be between 0.0 and 1.0, and the H value should range between 0.0 and 360.0. We will need to add 360.0 to any H value which is less than 0.0 and subtract 360.0 from any value greater than or equal to 360.0. \\n\\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This document describes a method for recognizing shapes using an R-table.  The R-table stores information about the orientation and distance of edge points from a reference point.  To create the R-table, each edge point is analyzed to determine its orientation (phi) and distance (r) from the reference point.  The orientation of the line from the reference point to the edge point (alpha) is also determined.  This information is then stored in the R-table as an (r, alpha) pair.  To recognize a shape, the R-table is compared to the R-tables of known shapes.  This process is done by accumulating evidence in a Hough space for the likelihood of a given shape being present. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n",
            "Topics: ['Image Segmentation Techniques', 'Object Detection Techniques', 'Deep Learning for Image Segmentation', 'Deep Learning for Object Detection', 'Traditional Image Segmentation Methods', 'Traditional Object Detection Methods', 'Semantic Segmentation', 'Instance Segmentation', 'Panoptic Segmentation', 'Object Localization', 'Object Classification', 'Image Preprocessing for Segmentation and Detection', 'Evaluation Metrics for Segmentation and Detection', 'Applications of Image Segmentation and Object Detection']\n",
            "Docs: ['The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The Gaussian Mixture Model (GMM) is a widely used approach for background subtraction in computer vision. It represents the background as a mixture of Gaussian distributions, each representing a different background appearance. The weight of each Gaussian determines how the averages are combined to form the background image.  New distributions are created for foreground pixels, and the least relevant distribution is discarded when the maximum number of distributions is reached. This method is unsupervised, meaning it learns the weights, averages, and standard deviations from the observed data. OpenCV provides the BackgroundSubtractorMOG2 class for implementing GMM background subtraction. \\n', ' To determine the median value for each pixel, we must update the histogram for each pixel every time a new frame is processed. This requires removing the oldest value and adding the new value.  This is expensive from a memory point of view as it requires storing m frames and their corresponding histograms.  Therefore, we must determine an appropriate value for m and limit the quantization in the histograms to prevent excessive data storage, especially for color images. To update the histograms efficiently, a form of aging can be used. \\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", ' An integral image is a representation of an image where each pixel stores the sum of all pixel values above and to the left of it.  This allows for efficient computation of the sum of pixels within any rectangular region of the image.  This is useful for feature detection, where we need to quickly compute the sum of pixels within a rectangular region to extract features like edges or corners.  For object detection, thousands of features are evaluated, and AdaBoost is used to combine weak classifiers based on these features to create a strong classifier.  This strong classifier can then be used to detect objects in new images. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', '64 A Practical IntroductiontoComputer Vision with OpenCV Figure 4 17 A binary image  top right  which has been determined through subtracting the current image topleft fromabackgroundimage convertingtogrey scaleandthresholding Thatbinaryimage is closed to fill in holes and bridges small gaps  bottom left   and then is opened to remove small and narrow regions  bottom right   The original image  top left  is reproduced by permission of Dr  James Ferryman  University of Reading Figure 4 18 A binary image  top left  which has been closed to join close objects  middle  and then opened to break thin connections  right   Note that one of the tracks  bottom middle  of the printed circuit board has broken during the opening operation  which may indicate that there is a problem with the board', 'To convert RGB to HLS, we use the following formulas, assuming R, G, and B have been normalized to between 0.0 and 1.0:\\n\\nL = (Max(R, G, B) + Min(R, G, B)) / 2\\nS = (Max(R, G, B) - Min(R, G, B)) / (Max(R, G, B) + Min(R, G, B)) if L <= 0.5\\nS = (Max(R, G, B) - Min(R, G, B)) / (2 - Max(R, G, B) - Min(R, G, B)) if L > 0.5\\n\\nH = 60 * (G - B) / S if R == Max(R, G, B)\\nH = 120 + 60 * (B - R) / S if G == Max(R, G, B)\\nH = 240 + 60 * (R - G) / S if B == Max(R, G, B)\\n\\nFrom the above formulas, the L and S values will be between 0.0 and 1.0, and the H value should range between 0.0 and 360.0. We will need to add 360.0 to any H value which is less than 0.0 and subtract 360.0 from any value greater than or equal to 360.0. \\n\\n\\n', \"This chapter focuses on face recognition using OpenCV. The technique utilizes a cascade of classifiers to gradually eliminate irrelevant objects, leaving only the desired ones. This method effectively reduces computational time by rejecting most negative sub-images in the initial stages. The classifiers within the cascade are trained using AdaBoost, ensuring a low false negative rate. Viola and Jones's frontal face detection cascade, with 38 stages and over 6000 features, is a notable example. It was trained using 4916 positive and 9544 negative example images.  The system processes images at various scales, as indicated by the varying rectangle sizes. Other recognition techniques, including Support Vector Machines (SVM), are supported by OpenCV and offer alternative approaches to statistical pattern recognition. \\n\", 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n', 'The current frame (left) shows the running average with selective update. The background image is updated with α = 0.01 for background points and α = 0.0033 for foreground points. The detected moving pixels are highlighted in the center, and the original frame is on the right. Compared to the previous frame, the car on the right has been effectively averaged out of the background, and the parked car is slowly being averaged into the background. The background model is a grayscale image. A color version of this model is available in the resources accompanying the text. The original images are from the PETS 2000 dataset, reproduced by permission of Dr. James Ferryman, University of Reading. \\n\\n\\n', 'This edition was first published in 2014 by John Wiley & Sons Ltd. All rights reserved. No part of this publication may be reproduced, stored in a retrieval system or transmitted in any form or by any means, electronic, mechanical, photocopying, recording or otherwise, except as permitted by the UK Copyright, Designs and Patents Act 1988, without the prior permission of the publisher.  Wiley also publishes its books in a variety of electronic formats. Some content that appears in print may not be available in electronic books. The publisher is not associated with any product or vendor mentioned in this book. \\n', 'Computer vision is becoming increasingly prevalent in our world, with applications ranging from driver assistance systems to medical diagnosis.  While the potential benefits are enormous, there are also significant challenges to be overcome. One challenge is legal liability. Who is to blame if a self-driving car crashes?  Similarly, who is responsible if a medical imaging system fails to diagnose cancer?  Another challenge is privacy. The widespread use of cameras raises concerns about surveillance and the potential for misuse of data.  Despite these challenges, the field of computer vision is rapidly advancing. We can expect to see computer vision systems become more sophisticated and capable of handling increasingly complex tasks in the years to come.  These tasks include recognizing objects of different types, extracting reliable descriptions of the world, and providing increased levels of security through biometric analysis.  Ultimately, computer vision has the potential to revolutionize the way we interact with the world around us. \\n', 'I am grateful to many people for their help and support during the writing of this book. The biggest thanks must go to my wife Jane, my children William and Susie, and my parents, all of whose encouragement has been unstinting. I must express my thanks to my students for their interest and enthusiasm in this subject. It is always refreshing to hear students discussing how to solve vision problems in tutorials and great to hear their solutions to problems which are often different, and sometimes better, than my own. I thank my colleagues, in particular Arthur Hughes, Jeremy Jones, and Hilary McDonald, for their encouragement and support. \\n']\n"
          ]
        }
      ]
    }
  ]
}